{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS notebook is based on C:\\SCRIPTS/RF_imageClass_MPB_p27_FT_3.py\n",
    "#environment space is located in \"img_proc2\"\n",
    "#November 2020\n",
    "#NASA ARSET: Land Cover Classification with Radar and Optical Data, Part 2/4\n",
    "#https://www.youtube.com/watch?v=raXA3gnb94Q  is RF on Google Earth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os,  numpy as np\n",
    "# from gdalconst import *\n",
    "# import pandas as pd\n",
    "# # import gdal,sys, time\n",
    "# from sklearn import metrics\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# #for visualization\n",
    "# import seaborn as sn\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Set random seed. if it is not set up then every run would produce different set of important bands\n",
    "# np.random.seed(0)\n",
    "# %matplotlib inline\n",
    "# A list of \"random\" colors (for a nicer output)\n",
    "COLORS = [\"#000000\", \"#FFFF00\", \"#1CE6FF\", \"#FF34FF\", \"#FF4A46\", \"#008941\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for accuracy assessment\n",
    "\n",
    "from sklearn.metrics import  accuracy_score, f1_score,confusion_matrix, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create some functions\n",
    "to process vector files, write geotiffs and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_vector(vector_data_path, cols, rows, geo_transform, projection, target_value=1):\n",
    "    \"\"\"Rasterize the given vector (wrapper for gdal.RasterizeLayer).\"\"\"\n",
    "    data_source = gdal.OpenEx(vector_data_path, gdal.OF_VECTOR)\n",
    "    layer = data_source.GetLayer(0)                                #???????????????????????\n",
    "    featureCount = layer.GetFeatureCount()\n",
    "    print(\"Number of features in %s: %d\" % (vector_data_path, featureCount))\n",
    "    driver = gdal.GetDriverByName('MEM')  # In memory dataset\n",
    "    #target_ds = driver.Create('', cols, rows, 1, gdal.GDT_UInt16)\n",
    "    target_ds = driver.Create('', cols, rows, 1,gdal.GDT_Float32)\n",
    "    target_ds.SetGeoTransform(geo_transform)\n",
    "    target_ds.SetProjection(projection)\n",
    "    gdal.RasterizeLayer(target_ds, [1], layer, burn_values=[target_value])\n",
    "    return target_ds\n",
    "\n",
    "\n",
    "def vectors_to_raster(file_paths, rows, cols, geo_transform, projection):\n",
    "    \"\"\"Rasterize all the vectors in the given directory into a single image. all\n",
    "    vectors get value of order they are in .\"\"\"\n",
    "    labeled_pixels = np.zeros((rows, cols))\n",
    "    for i, path in enumerate(file_paths):\n",
    "        \n",
    "        label = i+1\n",
    "        #print(\"label:\", label, \"path:\", path)\n",
    "        ds = create_mask_from_vector(path, cols, rows, geo_transform, projection, target_value=label)\n",
    "        band = ds.GetRasterBand(1)\n",
    "        labeled_pixels += band.ReadAsArray()\n",
    "        ds = None\n",
    "        #print(\"lable pixel:\", labeled_pixels)\n",
    "    return labeled_pixels\n",
    "\n",
    "\n",
    "def write_geotiff(fname, data, geo_transform, projection):\n",
    "    \"\"\"Create a GeoTIFF file with the given data.\"\"\"\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    rows, cols = data.shape\n",
    "    dataset = driver.Create(fname, cols, rows, 1, gdal.GDT_Byte)\n",
    "    dataset.SetGeoTransform(geo_transform)\n",
    "    dataset.SetProjection(projection)\n",
    "    band = dataset.GetRasterBand(1)\n",
    "    band.WriteArray(data)\n",
    "    dataset = None  # Close the file\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    \"evaluate accuracy of the model\"\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. set up paths and open the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(r\"C:/_LOCALdata/_PROJECTS_hist/ASLC/2020/F20\")\n",
    "print(\"wdir is:\", os.getcwd())\n",
    "# gdalbuildvrt -separate RGB.vrt red.tif green.tif blue.tif\n",
    "# gdal_translate RGB.vrt RGB.tif\n",
    "#raster_data_path = r\"D:\\...\\image_\\LC08_048020_20190616_S1_slpTPITRI_sub_1.tif\"\n",
    "#raster_data_path = r\"C:/....0/image/F_20_stack_virt\"\n",
    "#stack with Alos2 intensity\n",
    "#raster_data_path = r\"C:/_L....020/F20/image/F_20_stack_Alos2_int_virt\"\n",
    "#raster_data_path = r\"C:\\_LO......age\\Optic\\a_Sent_2_4_alos_26911_20m.tif\"\n",
    "raster_data_path = r\"C:/_L.....mage/F_20_stack_new1.tif\"\n",
    "t_fname = \"clas_pilot.tif\"\n",
    "red_output = \"clas_red_pilot.tif\"\n",
    "#train folder has got shapefiles with vector data to be used for training\n",
    "train_data_path = r\"C:/_L...../vectors/Alos_train_1\"\n",
    "\n",
    "#test folder  is similar to the train directory, but this samples are going to be used to\n",
    "#verify the classification results.\n",
    "validation_data_path = r\"C:/_.....ctors/Alos_test_1\"\n",
    "\n",
    "#raster_dataset = gdal.Open(raster_data_path, GA_ReadOnly)\n",
    "raster_dataset = gdal.Open(raster_data_path)\n",
    "if raster_dataset is None:\n",
    "    print ('Unable to open INPUT.tif')\n",
    "    sys.exit(1)\n",
    "cols = raster_dataset.RasterXSize\n",
    "#raster_dataset = gdal.Open(raster_data_path, gdal.GA_ReadOnly)\n",
    "geo_transform = raster_dataset.GetGeoTransform()\n",
    "proj = raster_dataset.GetProjectionRef()\n",
    "numbOfBands = raster_dataset.RasterCount\n",
    "print(\"#ofbands:\" , numbOfBands)\n",
    "print(\"cols:\",cols)\n",
    "print(\"gtr:\",geo_transform)\n",
    "print(\"projection:\", proj)\n",
    "bands_data = []\n",
    "#exclude 1st L8 \"Coastal band\"\n",
    "#for b in range( raster_dataset.RasterCount):  #exclude 1st band\n",
    "#read data into a stack.....\n",
    "for b in range(    numbOfBands):\n",
    "    print(\"b_:\", b, \"=\", b+1)\n",
    "\n",
    "    band = raster_dataset.GetRasterBand(b+1)\n",
    "    bands_data.append(band.ReadAsArray())\n",
    "\n",
    "bands_data = np.dstack(bands_data)\n",
    "rows, cols, n_bands = bands_data.shape\n",
    "print(\"row,col,band:\",rows, cols, n_bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a process training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files = [f for f in os.listdir(train_data_path) if f.endswith('.shp')]\n",
    "classes = [f.split('.')[0] for f in files]\n",
    "shapefiles = [os.path.join(train_data_path, f)  for f in files if f.endswith('.shp')]\n",
    "print(\"shp:\", shapefiles)\n",
    "\n",
    "#create classes\n",
    "labeled_pixels = vectors_to_raster(shapefiles, rows, cols, geo_transform, proj)\n",
    "is_train = np.nonzero(labeled_pixels)\n",
    "#show row and col index for each label pixel\n",
    "print(\"is_train:\", is_train)\n",
    "training_labels = labeled_pixels[is_train]   #show the value of the label pixel or agcc class\n",
    "#print(\"tr_labe:\",training_labels)\n",
    "\n",
    "#show value of the  pixels in each band under training polygon\n",
    "training_samples = bands_data[is_train]\n",
    "#print(\"tr_samp:\",training_samples )\n",
    "# print(\"tl:\", training_labels.shape)\n",
    "# print(\"ts:\",training_samples.shape)\n",
    "\n",
    "training_labels\n",
    "clase = np.unique(training_labels)\n",
    "print(\"trainig includes {n}  classes with codes: {classes}  \" .format (n= clase.size, classes= clase))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b ...and process validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************process validation (Test) files\n",
    "# they will be used to Assess the results\n",
    " #put valid data into a list\n",
    "    \n",
    "v_files = [fv for fv in os.listdir(validation_data_path) if fv.endswith('.shp')] \n",
    "#v_shapefiles = [os.path.join(validation_data_path, \"%s.shp\"%c) for c in classes]\n",
    "v_classes = [fv.split('.')[0] for fv in v_files]\n",
    "v_shapefiles = [os.path.join(validation_data_path, fv) for fv in v_files if fv.endswith('.shp')]\n",
    "print(v_shapefiles)\n",
    "\n",
    "verification_pixels = vectors_to_raster(v_shapefiles, rows, cols, geo_transform, proj)\n",
    "is_test = np.nonzero(verification_pixels) #2 arrays one for cols and another for rows pointing test samples\n",
    "testing_labels = verification_pixels[is_test]   #show the value of the label pixel or agcc class\n",
    "print(\"test_labe:\",testing_labels)\n",
    "#pixel values for each important band undet test sample\n",
    "#TESTING data -- pixel values oftesting samples......\n",
    "testing_samples = bands_data[is_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Initiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(random_state=42)\n",
    "#PARAMS\n",
    "#********************  train the bse model   ****************\n",
    "classifier.fit(training_samples, training_labels)  #\n",
    "\n",
    "##Make Predictions on Test Data for base model\n",
    "\n",
    "predicted_labels_0 = classifier.predict(testing_samples )\n",
    "## Determine Performance Metrics\n",
    "matrix = confusion_matrix(testing_labels, predicted_labels_0)\n",
    "print('Confusion matrix : \\n')\n",
    "print( matrix)\n",
    "# outcome values order in sklearn\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "matrix_rep = classification_report(testing_labels, predicted_labels_0)\n",
    "print('Classification report : ' )\n",
    "print(matrix_rep)\n",
    "\n",
    "print('initial_Correct red Prediction (%): ', accuracy_score(testing_labels,predicted_labels_0, normalize=True)*100.0)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. FEATURE IMPORTANCES\n",
    " finds which raster input bands are important. those corelated with others are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################   Variable Importances\n",
    "#our model is trained now on training data! MOdel can be trained on entire data as well\n",
    "######################################################\n",
    "print(\"#ofbands:\" , n_bands)\n",
    "listOfBands = []\n",
    "for i  in range(1, numbOfBands +1):\n",
    "    print(\"band:\", i)\n",
    "   # listOfBands.append(i+1)\n",
    "    listOfBands.append(i )\n",
    "print(\"lofB:\", listOfBands)\n",
    "\n",
    "for b, imp in zip(listOfBands,classifier.feature_importances_):\n",
    "    print(\"band: {b} importance: {imp}\" .format(b=b, imp=imp))\n",
    "\n",
    "#What bands are the most important for clasified output\n",
    "# Get numerical feature importances\n",
    "importances = list(classifier.feature_importances_  ) #put in list feature_importances_\n",
    "#\n",
    "# # List of tuples with variable and importance\n",
    "\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(listOfBands, importances)]\n",
    "\n",
    "# # Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# # Print out the feature and importances\n",
    "for pair in feature_importances:\n",
    "    print('Variable: {:10} Importance: {}'.format(*pair))\n",
    "#[print('Variable: {:10} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "# #**************** Create a plot of most important bands**********************************\n",
    "# #***********************************************************************************\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(1,len(importances)+1))\n",
    "print(\"xval:\",x_values)\n",
    "# # Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n",
    "\n",
    "# # Tick labels for x axis\n",
    "plt.xticks(x_values, listOfBands, rotation='horizontal')\n",
    "\n",
    "# # Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('bands'); plt.title('Variable Importances');\n",
    "plt.show()\n",
    "plt.savefig('bars_S2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. SORTED IMPORTANCES \n",
    "now we sort them according to importance. the least important are thrown away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in feature_importances]\n",
    "sorted_features = [importance[0] for importance in feature_importances]\n",
    "print(\"sorted importances:\",sorted_importances)\n",
    "print(\"sorted bands:\",sorted_features)  #\n",
    "# Cumulative importances\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "print(\"ci:\",cumulative_importances )\n",
    "# Make a line graph\n",
    "plt.plot(x_values, cumulative_importances, 'g-')\n",
    "print(x_values)\n",
    "# # Draw line at 95% of importance retained\n",
    "plt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n",
    "\n",
    "# # Format x ticks and labels\n",
    "plt.xticks(x_values, sorted_features, rotation = 'vertical')\n",
    "\n",
    "# # Axis labels and title\n",
    "plt.xlabel('bands'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');\n",
    "plt.show()\n",
    "plt.savefig('cumusumm_S2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. DATA REDUCTION\n",
    "now we have created new dataset with less bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of features for cumulative importance of 95%\n",
    "# Add 1 because Python is zero-indexed\n",
    "needed_bands = np.where(cumulative_importances > 0.95)[0][0] + 1\n",
    "print('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1)\n",
    "print(\"band:\",raster_dataset.RasterCount)\n",
    "print(\"nb:\",needed_bands)\n",
    "if (needed_bands < raster_dataset.RasterCount  ):\n",
    "    print(\" need for data reduction          \")\n",
    "else:\n",
    "    print(\" no data reduction necessary\")\n",
    "# # Extract the names of the most important features\n",
    "# CHANGE NUMBER OF FEATURES FOR 95 IMPORTANCES [0:????]\n",
    "important_feature_names = [feature[0] for feature in feature_importances[0:needed_bands]]\n",
    "# Find the columns of the most important features\n",
    "important_indices = [listOfBands.index(feature) + 1 for feature in listOfBands]\n",
    "# feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(bands, importances)]\n",
    "#print(\"imp:\", important_indices)\n",
    "print(\"imp name:\", important_feature_names)\n",
    "alist = []\n",
    "for a in range(needed_bands):\n",
    "   # print(\"aband: \",important_feature_names [a])\n",
    "    alist.append(important_feature_names [a])\n",
    "alist = sorted(alist)\n",
    "\n",
    "imp_bands_data = []\n",
    "#for bs in important_feature_names:  # exclude 1st band\n",
    "for bs in alist:\n",
    "    iband = raster_dataset.GetRasterBand(bs)\n",
    "    ar = iband.ReadAsArray()\n",
    "    imp_bands_data.append(ar)\n",
    "\n",
    "#new stack with important bands is \"imp_bands_data\"\n",
    "imp_bands_data = np.dstack(imp_bands_data)\n",
    "row, col, imp_bands = imp_bands_data.shape\n",
    "print(\"imp_shp:\", imp_bands_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Training the RF model on reducted data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "#show row and col index for each label pixel #show value of the  pixels in each band under training polygon\n",
    "#*********RUN CLASIFIER ON TRAINING DATA WITH REDUCTED DATASET***********************\n",
    "#LABELS  are the same , but imp_traing pixels are different due to less number of bands\n",
    "\n",
    "print(\"train_lab_shp:\",training_labels.shape)\n",
    "imp_training_samples = imp_bands_data[is_train]\n",
    "imp_testing_samples = imp_bands_data[is_test]\n",
    "print(\"test & train_sampl_shp:\",imp_training_samples.shape, imp_testing_samples.shape)\n",
    "# #***********************\n",
    "###        Model with The Most Important Features***************\n",
    "#START model on feature enginered*******************************************************************************************************************************\n",
    "\n",
    "classifier_imp = RandomForestClassifier(criterion = 'entropy',max_features= 'sqrt', n_jobs=-1,n_estimators=100,oob_score=True)\n",
    "classifier_imp.fit(imp_training_samples, training_labels)\n",
    "predicted_imp_labels = classifier_imp.predict(imp_testing_samples )\n",
    "print('initial_Correct import Prediction (%): ', accuracy_score(testing_labels,predicted_imp_labels, normalize=True)*100.0)\n",
    "accur_4_imp = evaluate(classifier_imp,imp_testing_samples,testing_labels)\n",
    "accur_4_base = evaluate(classifier, training_samples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters for classifier currently in use:\\n')\n",
    "pprint(classifier.get_params())\n",
    "print('Parameters for IMP classifier currently in use:\\n')\n",
    "pprint(classifier_imp.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  10. FINE TUNING of the model\n",
    "we try to find best parameters for RF model using\n",
    "**Random Search with Cross Validation**  producing highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print( \"Crosswalidation...Use the random grid to search for best hyperparameters.\")\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 200, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(\"these is random_grid parameters\")\n",
    "pprint(random_grid)\n",
    "# First create the base model to tune\n",
    "tune_classifier_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation,\n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator=tune_classifier_rf, param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='neg_mean_absolute_error',\n",
    "                              cv = 3, verbose=2, random_state=42, n_jobs=-1,  return_train_score=True)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(imp_training_samples, training_labels);\n",
    "print(\"these are the best params random search....\")\n",
    "print( rf_random.best_params_)\n",
    "#print long list of results\n",
    "#rf_random.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate the Default Model\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "print(\"after grid search evaluation of th best random model: \")\n",
    "random_accuracy = evaluate(best_random, imp_testing_samples, testing_labels)\n",
    "print(\"rand_acuur: \",random_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. GRID SEARCH \n",
    "Now, we do grid search based on best parameters from RandomizedSearch. \n",
    "We test a range of hyperparameters around best values returned from randomsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################*********2*****************************\n",
    "# Create the parameter grid based on the results of random search\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [ 90, 100, 110, 120],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [70, 80, 90, 110]\n",
    "}\n",
    "# Create a base model\n",
    "rf_gs =RandomForestClassifier(random_state = 42)\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf_gs, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2, return_train_score=True)\n",
    "grid_search.fit(imp_training_samples, training_labels);\n",
    "\n",
    "print(\"grid_search.best_params_:\",grid_search.best_params_)\n",
    "#Evaluate the Best Model from Grid Search\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "gs_accuracy = evaluate(best_grid , imp_testing_samples, testing_labels)\n",
    "print(\"gs_acuur: \",gs_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. AUC_ROC curve for multiclass classification\n",
    "it uses OneVsRestClassifier tehnique\n",
    "plots true positive rates vs false positive rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_imp2= OneVsRestClassifier(RandomForestClassifier(bootstrap= True, min_samples_leaf= 3,n_estimators= 90, min_samples_split= 8, max_features = 2, max_depth= 90))\n",
    "\n",
    "#classifier_imp2=clf = OneVsRestClassifier(RandomForestClassifier())#for ROC curve\n",
    "classifier_imp2.fit(imp_training_samples, training_labels)  #\n",
    "#classifier_imp2= OneVsRestClassifier(best_grid)\n",
    "predicted_labels_roc = classifier_imp2.predict(imp_testing_samples )\n",
    "pred_prob_labels_roc = classifier_imp2.predict_proba(imp_testing_samples )\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "n_class = len(v_files)\n",
    "for i in range(len(v_files)):\n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(testing_labels, pred_prob_labels_roc[:, i], pos_label=i)\n",
    "\n",
    "# plotting\n",
    "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Dec_55 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='graminoid_82 vs Rest')\n",
    "plt.plot(fpr[2], tpr[2], linestyle='--',color='black', label='mix_conifer_54 vs Rest')\n",
    "plt.plot(fpr[3], tpr[3], linestyle='--',color='cyan', label='Sb_186 vs Rest')\n",
    "plt.plot(fpr[4], tpr[4], linestyle='--',color='red', label='Pj_52 vs Rest')\n",
    "plt.plot(fpr[5], tpr[5], linestyle='--',color='brown', label='Sb_51 vs Rest')\n",
    "plt.plot(fpr[6], tpr[6], linestyle='--',color='magenta', label='bog_86 vs Rest')\n",
    "plt.plot(fpr[7], tpr[7], linestyle='--',color='maroon', label='lichen_87 vs Rest')\n",
    "plt.plot(fpr[8], tpr[8], linestyle='--',color='yellow', label='burn_200 vs Rest')\n",
    "plt.plot(fpr[9], tpr[9], linestyle='--',color='blue', label='water_80 vs Rest')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Multiclass ROC',dpi=300);\n",
    "#\n",
    "# ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"our reducted OOB prediction of accuracy for clasifier_imp is {oob}% \" .format(oob=classifier_imp.oob_score_*100))\n",
    "\n",
    "print('initial_Correct red Prediction (%): ', accuracy_score(testing_labels,predicted_imp_labels, normalize=True)*100.0)\n",
    "#print('Correct Prediction (%): ', accuracy_score(testing_labels,predicted_labels, normalize=True)*100.0)\n",
    "#*********************************************************************************************************\n",
    "df=pd.DataFrame()\n",
    "df['truth'] =testing_labels\n",
    "df['predict']=predicted_imp_labels\n",
    "\n",
    "#crosstabulate predictions\n",
    "print(pd.crosstab(df['truth'],df['predict'],margins=True))\n",
    "\n",
    "# #confusion metrix***************************\n",
    "cm=confusion_matrix(testing_labels,predicted_imp_labels )\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.heatmap(cm, annot=True)\n",
    "plt.xlabel('predict')\n",
    "plt.ylabel('truth')\n",
    "plt.show()\n",
    "plt.savefig('cm.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#runing diferent number of trees to see accuracy effect\n",
    "trees = range(180)\n",
    "accuracy = np.zeros(180)\n",
    "for i in range(len(trees)):\n",
    "    classifier_imp = RandomForestClassifier( n_estimators=i + 1)\n",
    "    #classifier1=classifier1.fit(training_samples, training_labels)  #\n",
    "    classifier_imp.fit(imp_training_samples, training_labels)\n",
    "    #prediction = classifier_imp.predict(imp_testing_samples)\n",
    "    predicted_labels = classifier_imp.predict(imp_testing_samples)  #==predicted_imp_labels = classifier_imp.predict(imp_testing_samples )\n",
    "    accuracy[i]= accuracy_score(testing_labels,predicted_labels)\n",
    "\n",
    "plt.cla()\n",
    "plt.plot(trees,accuracy)\n",
    "plt.savefig('efectAccur.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. APPLY THE BEST MODEL TO ENTIRE IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for entire image**************************\n",
    "# we used trained object to clasify all the input image.. Predict function expects a list of poxels not\n",
    "# an matrix. #now, we use trained object (\"classifier\") to classify entire image\n",
    "# #\n",
    "n_samples = rows*cols\n",
    "\n",
    "flat_pixels = imp_bands_data.reshape((n_samples, needed_bands)) #3D image is reshaped into 2D\n",
    "print(\"fp_shp:\", flat_pixels.shape)\n",
    "pred_img = classifier_imp2.predict(flat_pixels)\n",
    "# #print(\"imp res:\"),pred_img\n",
    "#classifier_imp2\n",
    "classification = pred_img.reshape((rows, cols))\n",
    "write_geotiff(red_output,classification,geo_transform, proj)\n",
    "fin_pred_labels = classification[is_test]\n",
    "#*********************************************************************************************\n",
    "print(\"confussion matrix:\\n%s\" % metrics.confusion_matrix(testing_labels, fin_pred_labels))\n",
    "target_names = ['class %s' % s for s in v_classes]\n",
    "\n",
    "print(\"classification report:\\n%s\" % metrics.classification_report(testing_labels, fin_pred_labels, target_names=target_names))\n",
    "print(\"classification accuracy: %f\" % metrics.accuracy_score(testing_labels, fin_pred_labels))\n",
    "#\n",
    "\n",
    "#visualize data\n",
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "f = plt.figure()\n",
    "f.add_subplot(1,2,2)\n",
    "r = bands_data[:,:,5]\n",
    "g = bands_data[:,:,4]\n",
    "b = bands_data[:,:,3]\n",
    "rgb = np.dstack([r,g,b])\n",
    "f.add_subplot(1, 2, 1)\n",
    "plt.imshow(rgb)\n",
    "f.add_subplot(1, 2, 2)\n",
    "plt.imshow(classification)\n",
    "plt.savefig('Alosclassification.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
